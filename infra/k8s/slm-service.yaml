---
# Production deployment for the SomaLanguage Model (SLM) service (formerly somallm-provider)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: slm-service
  namespace: soma-agent-hub
  labels:
    app: slm-service
    component: inference
    monitoring: enabled
spec:
  replicas: 2
  selector:
    matchLabels:
      app: slm-service
  template:
    metadata:
      labels:
        app: slm-service
        component: inference
        monitoring: enabled
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "1001"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: slm-service
        image: somaagent/soma-slm-service:latest
        imagePullPolicy: IfNotPresent
        
        ports:
        - name: http
          containerPort: 1001
          protocol: TCP
        - name: metrics
          containerPort: 1001
          protocol: TCP
        
        env:
        # Real model configuration
        - name: MODEL_PATH
          value: "/models"
        - name: DEFAULT_MODEL
          value: "llama-3.2-1b"
        - name: MAX_CONCURRENT_REQUESTS
          value: "10"
        
        # Real GPU config (if available)
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        
        # Real observability
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://prometheus-prometheus.observability:9090"
        - name: OTEL_SERVICE_NAME
          value: "slm-service"
        - name: ENVIRONMENT
          value: "production"
        
        resources:
          requests:
            cpu: "2000m"
            memory: "4Gi"
          limits:
            cpu: "8000m"
            memory: "16Gi"
            # Real GPU allocation (uncomment if GPU available)
            # nvidia.com/gpu: "1"
        
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 30
          periodSeconds: 5
          timeoutSeconds: 5
          failureThreshold: 3
        
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false  # Models need write for cache
          runAsNonRoot: true
          runAsUser: 1000
          capabilities:
            drop:
            - ALL
        
        volumeMounts:
        - name: models
          mountPath: /models
          readOnly: true
        - name: cache
          mountPath: /app/cache
      
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: slm-service-models
      - name: cache
        emptyDir: {}

# ClusterIP Service exposing SLM HTTP port
---
apiVersion: v1
kind: Service
metadata:
  name: slm-service
  namespace: soma-agent-hub
  labels:
    app: slm-service
    monitoring: enabled
spec:
  type: ClusterIP
  selector:
    app: slm-service
  ports:
    - name: http
      port: 1001
      targetPort: 1001
      protocol: TCP
    - name: metrics
      port: 1001
      targetPort: 1001
      protocol: TCP

# Persistent volume claim for model storage
---
# Real PVC for model storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: slm-service-models
  namespace: soma-agent-hub
spec:
  accessModes:
  - ReadOnlyMany
  resources:
    requests:
      storage: 50Gi
  storageClassName: standard
